{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install scikit-learn\n",
    "%pip install nltk\n",
    "%pip install wordcloud\n",
    "%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab65217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"scitweets_export.tsv\", sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_all = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different feature extraction methods\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "\n",
    "# Create label for Task 1\n",
    "y_task1 = df['science_related']\n",
    "\n",
    "# 1. Count Vectorizer\n",
    "count_vec = CountVectorizer(max_features=5000)\n",
    "X_count = count_vec.fit_transform(df['text'])\n",
    "\n",
    "# 2. TF-IDF with more parameters\n",
    "tfidf_vec = TfidfVectorizer(max_features=5000, \n",
    "                           min_df=5, \n",
    "                           max_df=0.8, \n",
    "                           ngram_range=(1, 2))\n",
    "X_tfidf = tfidf_vec.fit_transform(df['text'])\n",
    "\n",
    "# 3. TF-IDF with preprocessing already done\n",
    "tfidf_processed = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf_processed = tfidf_processed.fit_transform(df['text'])\n",
    "\n",
    "# Compare feature extraction methods\n",
    "print(f\"Count Vectorizer Features: {X_count.shape}\")\n",
    "print(f\"TF-IDF Vectorizer Features: {X_tfidf.shape}\")\n",
    "print(f\"TF-IDF on Preprocessed Text Features: {X_tfidf_processed.shape}\")\n",
    "\n",
    "# Feature selection using Chi-squared\n",
    "selector_chi2 = SelectKBest(chi2, k=100)\n",
    "X_chi2 = selector_chi2.fit_transform(X_tfidf, y_task1)\n",
    "\n",
    "# Feature selection using Mutual Information\n",
    "selector_mi = SelectKBest(mutual_info_classif, k=100)\n",
    "X_mi = selector_mi.fit_transform(X_tfidf, y_task1)\n",
    "\n",
    "print(f\"\\nFeatures after Chi-squared selection: {X_chi2.shape}\")\n",
    "print(f\"Features after Mutual Information selection: {X_mi.shape}\")\n",
    "\n",
    "# Get and visualize the most important features\n",
    "chi2_selected_indices = selector_chi2.get_support(indices=True)\n",
    "mi_selected_indices = selector_mi.get_support(indices=True)\n",
    "\n",
    "chi2_feature_names = np.array(tfidf_vec.get_feature_names_out())[chi2_selected_indices]\n",
    "mi_feature_names = np.array(tfidf_vec.get_feature_names_out())[mi_selected_indices]\n",
    "\n",
    "# Plot top 20 features by importance\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "chi2_scores = selector_chi2.scores_[chi2_selected_indices]\n",
    "chi2_features_df = pd.DataFrame({'Feature': chi2_feature_names, 'Score': chi2_scores})\n",
    "chi2_features_df = chi2_features_df.sort_values('Score', ascending=False).head(20)\n",
    "sns.barplot(x='Score', y='Feature', data=chi2_features_df)\n",
    "plt.title('Top 20 Features - Chi-squared')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "mi_scores = selector_mi.scores_[mi_selected_indices]\n",
    "mi_features_df = pd.DataFrame({'Feature': mi_feature_names, 'Score': mi_scores})\n",
    "mi_features_df = mi_features_df.sort_values('Score', ascending=False).head(20)\n",
    "sns.barplot(x='Score', y='Feature', data=mi_features_df)\n",
    "plt.title('Top 20 Features - Mutual Information')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# We'll use the TF-IDF on preprocessed text for subsequent modeling\n",
    "X_selected = X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977586d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for all tasks\n",
    "df['task1_label'] = df['science_related']\n",
    "\n",
    "df_sci = df[df['science_related'] == 1].copy()\n",
    "df_sci['task2_label'] = ((df_sci['scientific_claim'] == 1.0) | (df_sci['scientific_reference'] == 1.0)).astype(int)\n",
    "df_sci['task3_label'] = df_sci[['scientific_claim', 'scientific_reference', 'scientific_context']].idxmax(axis=1)\n",
    "df_sci['task3_label'] = df_sci['task3_label'].map({\n",
    "    'scientific_claim': 0,\n",
    "    'scientific_reference': 1,\n",
    "    'scientific_context': 2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efcae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'report': report,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "\n",
    "# Get a dense version of our features for Gaussian NB\n",
    "X_dense = X_selected.toarray()\n",
    "\n",
    "# Split data for Task 1\n",
    "X_train_task1, X_test_task1, y_train_task1, y_test_task1 = train_test_split(\n",
    "    X_dense, df['task1_label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Compare different NB variants for Task 1\n",
    "nb_models = {\n",
    "    'Gaussian NB': GaussianNB(),\n",
    "    'Multinomial NB': MultinomialNB(),\n",
    "    'Complement NB': ComplementNB(),\n",
    "    'Bernoulli NB': BernoulliNB(),\n",
    "    'KNN'   : KNeighborsClassifier(n_neighbors=5),\n",
    "    'SVM'   : SVC(kernel='linear', C=1),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "}\n",
    "\n",
    "task1_results = {}\n",
    "print(\"Task 1: Science Related Classification\\n\" + \"=\"*40)\n",
    "for name, model in nb_models.items():\n",
    "    task1_results[name] = evaluate_model(\n",
    "        model, X_train_task1, X_test_task1, y_train_task1, y_test_task1, name\n",
    "    )\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea17aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize k-fold cross validation\n",
    "k_folds = 10\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Dictionary to store results\n",
    "cv_results = {}\n",
    "\n",
    "# Perform k-fold cross validation for each model\n",
    "for model_name, model in nb_models.items():\n",
    "    print(f\"Performing {k_folds}-fold cross-validation for {model_name}...\")\n",
    "    \n",
    "    # Initialize lists to store performance metrics for each fold\n",
    "    fold_accuracy = []\n",
    "    fold_precision = []\n",
    "    fold_recall = []\n",
    "    fold_f1 = []\n",
    "    \n",
    "    # For each fold\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_dense, y_task1)):\n",
    "        # Split data\n",
    "        X_train_fold, X_test_fold = X_dense[train_idx], X_dense[test_idx]\n",
    "        y_train_fold, y_test_fold = y_task1.iloc[train_idx], y_task1.iloc[test_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_fold = model.predict(X_test_fold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_test_fold, y_pred_fold)\n",
    "        prec = precision_score(y_test_fold, y_pred_fold, zero_division=0)\n",
    "        rec = recall_score(y_test_fold, y_pred_fold, zero_division=0)\n",
    "        f1 = f1_score(y_test_fold, y_pred_fold, zero_division=0)\n",
    "        \n",
    "        fold_accuracy.append(acc)\n",
    "        fold_precision.append(prec)\n",
    "        fold_recall.append(rec)\n",
    "        fold_f1.append(f1)\n",
    "    \n",
    "    # Store average metrics and standard deviations\n",
    "    cv_results[model_name] = {\n",
    "        'accuracy': {\n",
    "            'mean': np.mean(fold_accuracy),\n",
    "            'std': np.std(fold_accuracy)\n",
    "        },\n",
    "        'precision': {\n",
    "            'mean': np.mean(fold_precision),\n",
    "            'std': np.std(fold_precision)\n",
    "        },\n",
    "        'recall': {\n",
    "            'mean': np.mean(fold_recall),\n",
    "            'std': np.std(fold_recall)\n",
    "        },\n",
    "        'f1': {\n",
    "            'mean': np.mean(fold_f1),\n",
    "            'std': np.std(fold_f1)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"  Average: Accuracy={cv_results[model_name]['accuracy']['mean']:.4f} ± {cv_results[model_name]['accuracy']['std']:.4f}\")\n",
    "    print(f\"  Average: F1 Score={cv_results[model_name]['f1']['mean']:.4f} ± {cv_results[model_name]['f1']['std']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': [],\n",
    "    'Metric': [],\n",
    "    'Mean': [],\n",
    "    'Std': []\n",
    "})\n",
    "\n",
    "for model_name in cv_results:\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "        results_df = pd.concat([results_df, pd.DataFrame({\n",
    "            'Model': [model_name],\n",
    "            'Metric': [metric.capitalize()],\n",
    "            'Mean': [cv_results[model_name][metric]['mean']],\n",
    "            'Std': [cv_results[model_name][metric]['std']]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "# Sort models by accuracy\n",
    "model_order = results_df[results_df['Metric'] == 'Accuracy'].sort_values('Mean', ascending=False)['Model'].tolist()\n",
    "\n",
    "# Create plots\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create bar plot for accuracy\n",
    "ax = sns.barplot(\n",
    "    data=results_df[results_df['Metric'] == 'Accuracy'],\n",
    "    x='Model',\n",
    "    y='Mean',\n",
    "    order=model_order,\n",
    "    palette='Blues_d'\n",
    ")\n",
    "\n",
    "# Add error bars\n",
    "for i, model in enumerate(model_order):\n",
    "    row = results_df[(results_df['Model'] == model) & (results_df['Metric'] == 'Accuracy')].iloc[0]\n",
    "    ax.errorbar(\n",
    "        i, row['Mean'], yerr=row['Std'],\n",
    "        fmt='o', color='black', elinewidth=2, capsize=6\n",
    "    )\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, bar in enumerate(ax.patches):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width()/2,\n",
    "        bar.get_height() + 0.01,\n",
    "        f\"{bar.get_height():.3f}\",\n",
    "        ha='center',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.title(f'Model Accuracy Comparison with {k_folds}-Fold Cross Validation', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim([0.5, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a grouped bar chart for all metrics\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create grouped bar plot\n",
    "ax = sns.barplot(\n",
    "    data=results_df,\n",
    "    x='Model',\n",
    "    y='Mean',\n",
    "    hue='Metric',\n",
    "    order=model_order,\n",
    "    palette='Set2'\n",
    ")\n",
    "\n",
    "plt.title(f'Model Performance Metrics Comparison ({k_folds}-Fold CV)', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Metric', loc='lower right')\n",
    "plt.ylim([0.5, 1.0])\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97bf103",
   "metadata": {},
   "source": [
    "# Parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce806afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for model_name, model in nb_models.items():\n",
    "    print(f\"Performing Grid Search for {model_name}...\")\n",
    "    # Define the parameter grid\n",
    "    if model_name == 'Gaussian NB':\n",
    "        param_grid = {\n",
    "            'var_smoothing': np.\n",
    "            logspace(0, -9, num=100)\n",
    "        }\n",
    "    elif model_name == 'Multinomial NB':\n",
    "        param_grid = {\n",
    "            'alpha': np.logspace(-3, 3, num=100),\n",
    "            'fit_prior': [True, False]\n",
    "        }\n",
    "    elif model_name == 'Complement NB':\n",
    "        param_grid = {\n",
    "            'alpha': np.logspace(-3, 3, num=100),\n",
    "            'fit_prior': [True, False]\n",
    "        }\n",
    "    elif model_name == 'Bernoulli NB':\n",
    "        param_grid = {\n",
    "            'alpha': np.logspace(-3, 3, num=100),\n",
    "            'fit_prior': [True, False]\n",
    "        }\n",
    "    elif model_name == 'KNN':\n",
    "        param_grid = {\n",
    "            'n_neighbors': [3, 5, 7, 9],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan']\n",
    "        }\n",
    "    elif model_name == 'SVM':\n",
    "        param_grid = {\n",
    "            'C': np.logspace(-3, 3, num=100),\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "    elif model_name == 'Logistic Regression':\n",
    "        param_grid = {\n",
    "            'C': np.logspace(-3, 3, num=100),\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga']\n",
    "        }\n",
    "    elif model_name == 'Random Forest':\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='accuracy',\n",
    "        cv=StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42),\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train_task1, y_train_task1)\n",
    "\n",
    "    # Get the best parameters and best score\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # Evaluate the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model_accuracy = best_model.score(X_test_task1, y_test_task1)\n",
    "    print(f\"Test Accuracy with Best Parameters: {best_model_accuracy:.4f}\")\n",
    "\n",
    "    # Visualize parameter impact\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    \n",
    "    # Plot effect of parameters\n",
    "\n",
    "    if model_name == 'Gaussian NB':\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(data=results, x='param_var_smoothing', y='mean_test_score')\n",
    "        plt.xscale('log')\n",
    "        plt.title(f'Gaussian NB: Effect of var_smoothing on Accuracy')\n",
    "        plt.xlabel('var_smoothing (log scale)')\n",
    "        plt.ylabel('Mean Test Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    elif model_name == 'Multinomial NB':\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(data=results, x='param_alpha', y='mean_test_score', hue='param_fit_prior')\n",
    "        plt.xscale('log')\n",
    "        plt.title(f'Multinomial NB: Effect of alpha and fit_prior on Accuracy')\n",
    "        plt.xlabel('alpha (log scale)')\n",
    "        plt.ylabel('Mean Test Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    elif model_name == 'Complement NB':\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(data=results, x='param_alpha', y='mean_test_score', hue='param_fit_prior')\n",
    "        plt.xscale('log')\n",
    "        plt.title(f'Complement NB: Effect of alpha and fit_prior on Accuracy')\n",
    "        plt.xlabel('alpha (log scale)')\n",
    "        plt.ylabel('Mean Test Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    elif model_name == 'Bernoulli NB':\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(data=results, x='param_alpha', y='mean_test_score', hue='param_fit_prior')\n",
    "        plt.xscale('log')\n",
    "        plt.title(f'Bernoulli NB: Effect of alpha and fit_prior on Accuracy')\n",
    "        plt.xlabel('alpha (log scale)')\n",
    "        plt.ylabel('Mean Test Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    elif model_name == 'KNN':\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(data=results, x='param_n_neighbors', y='mean_test_score', hue='param_weights')\n",
    "        plt.title(f'KNN: Effect of n_neighbors and weights on Accuracy')\n",
    "        plt.xlabel('n_neighbors')\n",
    "        plt.ylabel('Mean Test Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    elif model_name == 'SVM':\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(data=results, x='param_C', y='mean_test_score', hue='param_kernel')\n",
    "        plt.xscale('log')\n",
    "        plt.title(f'SVM: Effect of C and kernel on Accuracy')\n",
    "        plt.xlabel('C (log scale)')\n",
    "        plt.ylabel('Mean Test Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    elif model_name == 'Logistic Regression':\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(data=results, x='param_C', y='mean_test_score', hue='param_penalty')\n",
    "        plt.xscale('log')\n",
    "        plt.title(f'Logistic Regression: Effect of C and penalty on Accuracy')\n",
    "        plt.xlabel('C (log scale)')\n",
    "        plt.ylabel('Mean Test Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    elif model_name == 'Random Forest':\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(data=results, x='param_n_estimators', y='mean_test_score', hue='param_max_depth')\n",
    "        plt.title(f'Random Forest: Effect of n_estimators and max_depth on Accuracy')\n",
    "        plt.xlabel('n_estimators')\n",
    "        plt.ylabel('Mean Test Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No visualization available for {model_name} model.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d14998",
   "metadata": {},
   "source": [
    "# Training comparaison with best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
    "\n",
    "# Create dictionary of models with their best parameters based on grid search results\n",
    "\n",
    "# Define models with their best parameters based on grid search results\n",
    "best_models = {\n",
    "    'Gaussian NB': GaussianNB(var_smoothing=0.0152), #ample value - update based on grid search\n",
    "    'Multinomial NB': MultinomialNB(alpha=0.4641, fit_prior=True),  # Example value\n",
    "    'Complement NB': ComplementNB(alpha=2.8480, fit_prior=True),  # Example value\n",
    "    'Bernoulli NB': BernoulliNB(alpha=0.17475, fit_prior=True),  # Example value\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=7, weights='uniform', metric='euclidean'),\n",
    "    'SVM': SVC(C=1.4174742, kernel='rbf', gamma='scale', probability=True),\n",
    "    'Logistic Regression': LogisticRegression(C=np.float64(4.328761281083062), penalty='l1', solver='saga', max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=5, min_samples_leaf=1)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "best_cv_results = {}\n",
    "\n",
    "# Perform k-fold cross validation for each model\n",
    "for model_name, model in best_models.items():\n",
    "    print(f\"Performing {k_folds}-fold cross-validation for {model_name} with best parameters...\")\n",
    "    \n",
    "    # Initialize lists to store performance metrics for each fold\n",
    "    fold_accuracy = []\n",
    "    fold_precision = []\n",
    "    fold_recall = []\n",
    "    fold_f1 = []\n",
    "    fold_auc = []\n",
    "    \n",
    "    # For each fold\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_dense, y_task1)):\n",
    "        # Split data\n",
    "        X_train_fold, X_test_fold = X_dense[train_idx], X_dense[test_idx]\n",
    "        y_train_fold, y_test_fold = y_task1.iloc[train_idx], y_task1.iloc[test_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_fold = model.predict(X_test_fold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_test_fold, y_pred_fold)\n",
    "        prec = precision_score(y_test_fold, y_pred_fold, zero_division=0)\n",
    "        rec = recall_score(y_test_fold, y_pred_fold, zero_division=0)\n",
    "        f1 = f1_score(y_test_fold, y_pred_fold, zero_division=0)\n",
    "        \n",
    "        # For AUC, we need probability estimates\n",
    "        try:\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_prob = model.predict_proba(X_test_fold)[:, 1]\n",
    "                auc_score = roc_auc_score(y_test_fold, y_prob)\n",
    "                fold_auc.append(auc_score)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        fold_accuracy.append(acc)\n",
    "        fold_precision.append(prec)\n",
    "        fold_recall.append(rec)\n",
    "        fold_f1.append(f1)\n",
    "    \n",
    "    # Store average metrics and standard deviations\n",
    "    best_cv_results[model_name] = {\n",
    "        'accuracy': {\n",
    "            'mean': np.mean(fold_accuracy),\n",
    "            'std': np.std(fold_accuracy)\n",
    "        },\n",
    "        'precision': {\n",
    "            'mean': np.mean(fold_precision),\n",
    "            'std': np.std(fold_precision)\n",
    "        },\n",
    "        'recall': {\n",
    "            'mean': np.mean(fold_recall),\n",
    "            'std': np.std(fold_recall)\n",
    "        },\n",
    "        'f1': {\n",
    "            'mean': np.mean(fold_f1),\n",
    "            'std': np.std(fold_f1)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if fold_auc:\n",
    "        best_cv_results[model_name]['auc'] = {\n",
    "            'mean': np.mean(fold_auc),\n",
    "            'std': np.std(fold_auc)\n",
    "        }\n",
    "        print(f\"  Average: AUC={best_cv_results[model_name]['auc']['mean']:.4f} ± {best_cv_results[model_name]['auc']['std']:.4f}\")\n",
    "    \n",
    "    print(f\"  Average: Accuracy={best_cv_results[model_name]['accuracy']['mean']:.4f} ± {best_cv_results[model_name]['accuracy']['std']:.4f}\")\n",
    "    print(f\"  Average: F1 Score={best_cv_results[model_name]['f1']['mean']:.4f} ± {best_cv_results[model_name]['f1']['std']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "best_results_df = pd.DataFrame({\n",
    "    'Model': [],\n",
    "    'Metric': [],\n",
    "    'Mean': [],\n",
    "    'Std': []\n",
    "})\n",
    "\n",
    "for model_name in best_cv_results:\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "        best_results_df = pd.concat([best_results_df, pd.DataFrame({\n",
    "            'Model': [model_name],\n",
    "            'Metric': [metric.capitalize()],\n",
    "            'Mean': [best_cv_results[model_name][metric]['mean']],\n",
    "            'Std': [best_cv_results[model_name][metric]['std']]\n",
    "        })], ignore_index=True)\n",
    "    if 'auc' in best_cv_results[model_name]:\n",
    "        best_results_df = pd.concat([best_results_df, pd.DataFrame({\n",
    "            'Model': [model_name],\n",
    "            'Metric': ['AUC'],\n",
    "            'Mean': [best_cv_results[model_name]['auc']['mean']],\n",
    "            'Std': [best_cv_results[model_name]['auc']['std']]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "# Sort models by accuracy\n",
    "best_model_order = best_results_df[best_results_df['Metric'] == 'Accuracy'].sort_values('Mean', ascending=False)['Model'].tolist()\n",
    "\n",
    "# Create plots\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create bar plot for accuracy\n",
    "ax = sns.barplot(\n",
    "    data=best_results_df[best_results_df['Metric'] == 'Accuracy'],\n",
    "    x='Model',\n",
    "    y='Mean',\n",
    "    order=best_model_order,\n",
    "    palette='Blues_d'\n",
    ")\n",
    "\n",
    "# Add error bars\n",
    "for i, model in enumerate(best_model_order):\n",
    "    row = best_results_df[(best_results_df['Model'] == model) & (best_results_df['Metric'] == 'Accuracy')].iloc[0]\n",
    "    ax.errorbar(\n",
    "        i, row['Mean'], yerr=row['Std'],\n",
    "        fmt='o', color='black', elinewidth=2, capsize=6\n",
    "    )\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, bar in enumerate(ax.patches):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width()/2,\n",
    "        bar.get_height() + 0.01,\n",
    "        f\"{bar.get_height():.3f}\",\n",
    "        ha='center',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.title(f'Model Accuracy Comparison with {k_folds}-Fold Cross Validation (Best Parameters)', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim([0.5, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a grouped bar chart for all metrics\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create grouped bar plot\n",
    "ax = sns.barplot(\n",
    "    data=best_results_df[best_results_df['Metric'] != 'AUC'],\n",
    "    x='Model',\n",
    "    y='Mean',\n",
    "    hue='Metric',\n",
    "    order=best_model_order,\n",
    "    palette='Set2'\n",
    ")\n",
    "\n",
    "plt.title(f'Model Performance Metrics Comparison with Best Parameters ({k_folds}-Fold CV)', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Metric', loc='lower right')\n",
    "plt.ylim([0.5, 1.0])\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a comparison table between original results and optimized results\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': [],\n",
    "    'Metric': [],\n",
    "    'Original Mean': [],\n",
    "    'Original Std': [],\n",
    "    'Optimized Mean': [],\n",
    "    'Optimized Std': [],\n",
    "    'Improvement': []\n",
    "})\n",
    "\n",
    "for model_name in best_cv_results:\n",
    "    if model_name in cv_results:\n",
    "        for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "            orig_mean = cv_results[model_name][metric]['mean']\n",
    "            orig_std = cv_results[model_name][metric]['std']\n",
    "            opt_mean = best_cv_results[model_name][metric]['mean']\n",
    "            opt_std = best_cv_results[model_name][metric]['std']\n",
    "            improvement = ((opt_mean - orig_mean) / orig_mean) * 100\n",
    "            \n",
    "            comparison_df = pd.concat([comparison_df, pd.DataFrame({\n",
    "                'Model': [model_name],\n",
    "                'Metric': [metric.capitalize()],\n",
    "                'Original Mean': [orig_mean],\n",
    "                'Original Std': [orig_std],\n",
    "                'Optimized Mean': [opt_mean],\n",
    "                'Optimized Std': [opt_std],\n",
    "                'Improvement': [improvement]\n",
    "            })], ignore_index=True)\n",
    "\n",
    "# Plot the improvement in accuracy\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Filter for accuracy only\n",
    "acc_comparison = comparison_df[comparison_df['Metric'] == 'Accuracy']\n",
    "acc_comparison = acc_comparison.sort_values('Improvement', ascending=False)\n",
    "\n",
    "# Create bar chart of improvements\n",
    "ax = sns.barplot(\n",
    "    data=acc_comparison,\n",
    "    x='Model',\n",
    "    y='Improvement',\n",
    "    palette='RdYlGn',\n",
    "    dodge=False\n",
    ")\n",
    "\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(ax.patches):\n",
    "    if bar.get_height() >= 0:\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width()/2,\n",
    "            bar.get_height() + 0.5,\n",
    "            f\"{bar.get_height():.2f}%\",\n",
    "            ha='center',\n",
    "            fontsize=10\n",
    "        )\n",
    "    else:\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width()/2,\n",
    "            bar.get_height() - 1.0,\n",
    "            f\"{bar.get_height():.2f}%\",\n",
    "            ha='center',\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "plt.title('Percentage Improvement in Accuracy After Parameter Optimization', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Improvement (%)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f8f40",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded98baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Set style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot distribution of science_related tweets\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.countplot(x='science_related', data=df)\n",
    "plt.title('Distribution of Science Related Tweets')\n",
    "plt.xlabel('Is Science Related')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Plot distribution of science subtypes for science-related tweets\n",
    "sci_df = df[df['science_related'] == 1]\n",
    "plt.subplot(2, 2, 2)\n",
    "subtypes = ['scientific_claim', 'scientific_reference', 'scientific_context']\n",
    "sns.barplot(x=subtypes, y=[sci_df[col].sum() for col in subtypes])\n",
    "plt.title('Distribution of Science Subtypes')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot text length distribution\n",
    "plt.subplot(2, 2, 3)\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "sns.histplot(data=df, x='text_length', hue='science_related', bins=50, kde=True)\n",
    "plt.title('Text Length Distribution by Class')\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot correlation between features\n",
    "plt.subplot(2, 2, 4)\n",
    "corr_cols = ['science_related', 'scientific_claim', 'scientific_reference', 'scientific_context']\n",
    "sns.heatmap(df[corr_cols].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Between Labels')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05872e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import emoji\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Demojize text\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'(http\\S+|www\\S+)', '', text)\n",
    "\n",
    "\n",
    "   \n",
    "    # Remove mentions and hashtags execept for the word uurekamag\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#eurekamag', 'eurekamag', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Rejoin\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Compare before and after\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original': df['text'].head(5),\n",
    "    'Preprocessed': df['processed_text'].head(5)\n",
    "})\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98112fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the impact of preprocessing\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Text length before and after preprocessing\n",
    "df['original_length'] = df['text'].apply(len)\n",
    "df['processed_length'] = df['processed_text'].apply(len)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data=df, x='original_length', color='blue', bins=50, kde=True, label='Original')\n",
    "sns.histplot(data=df, x='processed_length', color='red', bins=50, kde=True, label='Preprocessed')\n",
    "plt.title('Text Length Distribution: Before vs After Preprocessing')\n",
    "plt.xlabel('Text Length')\n",
    "plt.legend()\n",
    "\n",
    "# Word cloud for processed text\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(' '.join(df['processed_text']))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Preprocessed Tweets')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f09acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_all = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897acc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different feature extraction methods\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "\n",
    "# Create label for Task 1\n",
    "y_task1 = df['science_related']\n",
    "\n",
    "# 1. Count Vectorizer\n",
    "count_vec = CountVectorizer(max_features=5000)\n",
    "X_count = count_vec.fit_transform(df['processed_text'])\n",
    "\n",
    "# 2. TF-IDF with more parameters\n",
    "tfidf_vec = TfidfVectorizer(max_features=5000, \n",
    "                           min_df=5, \n",
    "                           max_df=0.8, \n",
    "                           ngram_range=(1, 2))\n",
    "X_tfidf = tfidf_vec.fit_transform(df['processed_text'])\n",
    "\n",
    "# 3. TF-IDF with preprocessing already done\n",
    "tfidf_processed = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf_processed = tfidf_processed.fit_transform(df['processed_text'])\n",
    "\n",
    "# Compare feature extraction methods\n",
    "print(f\"Count Vectorizer Features: {X_count.shape}\")\n",
    "print(f\"TF-IDF Vectorizer Features: {X_tfidf.shape}\")\n",
    "print(f\"TF-IDF on Preprocessed Text Features: {X_tfidf_processed.shape}\")\n",
    "\n",
    "# Feature selection using Chi-squared\n",
    "selector_chi2 = SelectKBest(chi2, k=100)\n",
    "X_chi2 = selector_chi2.fit_transform(X_tfidf, y_task1)\n",
    "\n",
    "# Feature selection using Mutual Information\n",
    "selector_mi = SelectKBest(mutual_info_classif, k=100)\n",
    "X_mi = selector_mi.fit_transform(X_tfidf, y_task1)\n",
    "\n",
    "print(f\"\\nFeatures after Chi-squared selection: {X_chi2.shape}\")\n",
    "print(f\"Features after Mutual Information selection: {X_mi.shape}\")\n",
    "\n",
    "# Get and visualize the most important features\n",
    "chi2_selected_indices = selector_chi2.get_support(indices=True)\n",
    "mi_selected_indices = selector_mi.get_support(indices=True)\n",
    "\n",
    "chi2_feature_names = np.array(tfidf_vec.get_feature_names_out())[chi2_selected_indices]\n",
    "mi_feature_names = np.array(tfidf_vec.get_feature_names_out())[mi_selected_indices]\n",
    "\n",
    "# Plot top 20 features by importance\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "chi2_scores = selector_chi2.scores_[chi2_selected_indices]\n",
    "chi2_features_df = pd.DataFrame({'Feature': chi2_feature_names, 'Score': chi2_scores})\n",
    "chi2_features_df = chi2_features_df.sort_values('Score', ascending=False).head(20)\n",
    "sns.barplot(x='Score', y='Feature', data=chi2_features_df)\n",
    "plt.title('Top 20 Features - Chi-squared')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "mi_scores = selector_mi.scores_[mi_selected_indices]\n",
    "mi_features_df = pd.DataFrame({'Feature': mi_feature_names, 'Score': mi_scores})\n",
    "mi_features_df = mi_features_df.sort_values('Score', ascending=False).head(20)\n",
    "sns.barplot(x='Score', y='Feature', data=mi_features_df)\n",
    "plt.title('Top 20 Features - Mutual Information')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# We'll use the TF-IDF on preprocessed text for subsequent modeling\n",
    "X_selected = X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af043518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'report': report,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "\n",
    "# Get a dense version of our features for Gaussian NB\n",
    "X_dense = X_selected.toarray()\n",
    "\n",
    "# Split data for Task 1\n",
    "X_train_task1, X_test_task1, y_train_task1, y_test_task1 = train_test_split(\n",
    "    X_dense, df['task1_label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define models with their best parameters based on grid search results\n",
    "nb_models = {\n",
    "    'Gaussian NB': GaussianNB(var_smoothing=0.0152), #ample value - update based on grid search\n",
    "    'Multinomial NB': MultinomialNB(alpha=0.4641, fit_prior=True),  # Example value\n",
    "    'Complement NB': ComplementNB(alpha=2.8480, fit_prior=True),  # Example value\n",
    "    'Bernoulli NB': BernoulliNB(alpha=0.17475, fit_prior=True),  # Example value\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=7, weights='uniform', metric='euclidean'),\n",
    "    'SVM': SVC(C=1.4174742, kernel='rbf', gamma='scale', probability=True),\n",
    "    'Logistic Regression': LogisticRegression(C=np.float64(4.328761281083062), penalty='l1', solver='saga', max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=5, min_samples_leaf=1)\n",
    "}\n",
    "\n",
    "task1_results = {}\n",
    "print(\"Task 1: Science Related Classification\\n\" + \"=\"*40)\n",
    "for name, model in nb_models.items():\n",
    "    task1_results[name] = evaluate_model(\n",
    "        model, X_train_task1, X_test_task1, y_train_task1, y_test_task1, name\n",
    "    )\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using preprocessed text for feature extraction\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
    "\n",
    "# Create TF-IDF features from preprocessed text\n",
    "tfidf_vec = TfidfVectorizer(max_features=5000, \n",
    "                           min_df=5, \n",
    "                           max_df=0.8, \n",
    "                           ngram_range=(1, 2))\n",
    "X_tfidf_processed = tfidf_vec.fit_transform(df['processed_text'])\n",
    "\n",
    "# Feature selection using Chi-squared\n",
    "selector_chi2 = SelectKBest(chi2, k=100)\n",
    "X_chi2_processed = selector_chi2.fit_transform(X_tfidf_processed, y_task1)\n",
    "\n",
    "# Get a dense version of our features for models that require it\n",
    "X_processed_dense = X_tfidf_processed.toarray()\n",
    "\n",
    "# Initialize k-fold cross validation\n",
    "k_folds = 10\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Define models with their best parameters based on grid search results\n",
    "best_models = {\n",
    "    'Gaussian NB': GaussianNB(var_smoothing=0.0152),\n",
    "    'Multinomial NB': MultinomialNB(alpha=0.4641, fit_prior=True),\n",
    "    'Complement NB': ComplementNB(alpha=2.8480, fit_prior=True),\n",
    "    'Bernoulli NB': BernoulliNB(alpha=0.17475, fit_prior=True),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=7, weights='uniform', metric='euclidean'),\n",
    "    'SVM': SVC(C=1.4174742, kernel='rbf', gamma='scale', probability=True),\n",
    "    'Logistic Regression': LogisticRegression(C=np.float64(4.328761281083062), penalty='l1', solver='saga', max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=5, min_samples_leaf=1)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "best_cv_results = {}\n",
    "\n",
    "# Perform k-fold cross validation for each model\n",
    "for model_name, model in best_models.items():\n",
    "    print(f\"Performing {k_folds}-fold cross-validation for {model_name} with best parameters on preprocessed text...\")\n",
    "    \n",
    "    # Initialize lists to store performance metrics for each fold\n",
    "    fold_accuracy = []\n",
    "    fold_precision = []\n",
    "    fold_recall = []\n",
    "    fold_f1 = []\n",
    "    fold_auc = []\n",
    "    \n",
    "    # For each fold\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_processed_dense, y_task1)):\n",
    "        # Split data\n",
    "        X_train_fold, X_test_fold = X_processed_dense[train_idx], X_processed_dense[test_idx]\n",
    "        y_train_fold, y_test_fold = y_task1.iloc[train_idx], y_task1.iloc[test_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_fold = model.predict(X_test_fold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_test_fold, y_pred_fold)\n",
    "        prec = precision_score(y_test_fold, y_pred_fold, zero_division=0)\n",
    "        rec = recall_score(y_test_fold, y_pred_fold, zero_division=0)\n",
    "        f1 = f1_score(y_test_fold, y_pred_fold, zero_division=0)\n",
    "        \n",
    "        # For AUC, we need probability estimates\n",
    "        try:\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                y_prob = model.predict_proba(X_test_fold)[:, 1]\n",
    "                auc_score = roc_auc_score(y_test_fold, y_prob)\n",
    "                fold_auc.append(auc_score)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        fold_accuracy.append(acc)\n",
    "        fold_precision.append(prec)\n",
    "        fold_recall.append(rec)\n",
    "        fold_f1.append(f1)\n",
    "    \n",
    "    # Store average metrics and standard deviations\n",
    "    best_cv_results[model_name] = {\n",
    "        'accuracy': {\n",
    "            'mean': np.mean(fold_accuracy),\n",
    "            'std': np.std(fold_accuracy)\n",
    "        },\n",
    "        'precision': {\n",
    "            'mean': np.mean(fold_precision),\n",
    "            'std': np.std(fold_precision)\n",
    "        },\n",
    "        'recall': {\n",
    "            'mean': np.mean(fold_recall),\n",
    "            'std': np.std(fold_recall)\n",
    "        },\n",
    "        'f1': {\n",
    "            'mean': np.mean(fold_f1),\n",
    "            'std': np.std(fold_f1)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if fold_auc:\n",
    "        best_cv_results[model_name]['auc'] = {\n",
    "            'mean': np.mean(fold_auc),\n",
    "            'std': np.std(fold_auc)\n",
    "        }\n",
    "        print(f\"  Average: AUC={best_cv_results[model_name]['auc']['mean']:.4f} ± {best_cv_results[model_name]['auc']['std']:.4f}\")\n",
    "    \n",
    "    print(f\"  Average: Accuracy={best_cv_results[model_name]['accuracy']['mean']:.4f} ± {best_cv_results[model_name]['accuracy']['std']:.4f}\")\n",
    "    print(f\"  Average: F1 Score={best_cv_results[model_name]['f1']['mean']:.4f} ± {best_cv_results[model_name]['f1']['std']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "best_results_df = pd.DataFrame({\n",
    "    'Model': [],\n",
    "    'Metric': [],\n",
    "    'Mean': [],\n",
    "    'Std': []\n",
    "})\n",
    "\n",
    "for model_name in best_cv_results:\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "        best_results_df = pd.concat([best_results_df, pd.DataFrame({\n",
    "            'Model': [model_name],\n",
    "            'Metric': [metric.capitalize()],\n",
    "            'Mean': [best_cv_results[model_name][metric]['mean']],\n",
    "            'Std': [best_cv_results[model_name][metric]['std']]\n",
    "        })], ignore_index=True)\n",
    "    if 'auc' in best_cv_results[model_name]:\n",
    "        best_results_df = pd.concat([best_results_df, pd.DataFrame({\n",
    "            'Model': [model_name],\n",
    "            'Metric': ['AUC'],\n",
    "            'Mean': [best_cv_results[model_name]['auc']['mean']],\n",
    "            'Std': [best_cv_results[model_name]['auc']['std']]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "# Sort models by accuracy\n",
    "best_model_order = best_results_df[best_results_df['Metric'] == 'Accuracy'].sort_values('Mean', ascending=False)['Model'].tolist()\n",
    "\n",
    "# Create plots\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create bar plot for accuracy\n",
    "ax = sns.barplot(\n",
    "    data=best_results_df[best_results_df['Metric'] == 'Accuracy'],\n",
    "    x='Model',\n",
    "    y='Mean',\n",
    "    order=best_model_order,\n",
    "    palette='Blues_d'\n",
    ")\n",
    "\n",
    "# Add error bars\n",
    "for i, model in enumerate(best_model_order):\n",
    "    row = best_results_df[(best_results_df['Model'] == model) & (best_results_df['Metric'] == 'Accuracy')].iloc[0]\n",
    "    ax.errorbar(\n",
    "        i, row['Mean'], yerr=row['Std'],\n",
    "        fmt='o', color='black', elinewidth=2, capsize=6\n",
    "    )\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, bar in enumerate(ax.patches):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width()/2,\n",
    "        bar.get_height() + 0.01,\n",
    "        f\"{bar.get_height():.3f}\",\n",
    "        ha='center',\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.title(f'Model Accuracy Comparison with Preprocessed Text ({k_folds}-Fold CV)', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim([0.5, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a grouped bar chart for all metrics\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create grouped bar plot\n",
    "ax = sns.barplot(\n",
    "    data=best_results_df[best_results_df['Metric'] != 'AUC'],\n",
    "    x='Model',\n",
    "    y='Mean',\n",
    "    hue='Metric',\n",
    "    order=best_model_order,\n",
    "    palette='Set2'\n",
    ")\n",
    "\n",
    "plt.title(f'Model Performance with Preprocessed Text ({k_folds}-Fold CV)', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Metric', loc='lower right')\n",
    "plt.ylim([0.5, 1.0])\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd6879",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Compare performance of SVM with preprocessed data and Bernoulli NB without preprocessed data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_data = [\n",
    "    {\n",
    "        'Model': 'Bernoulli NB (Raw Text)',\n",
    "        'Accuracy': best_cv_results['Bernoulli NB']['accuracy']['mean'],\n",
    "        'Precision': best_cv_results['Bernoulli NB']['precision']['mean'],\n",
    "        'Recall': best_cv_results['Bernoulli NB']['recall']['mean'],\n",
    "        'F1-Score': best_cv_results['Bernoulli NB']['f1']['mean']\n",
    "    },\n",
    "    {\n",
    "        'Model': 'SVM (Preprocessed Text)',\n",
    "        'Accuracy': best_cv_results['SVM']['accuracy']['mean'],\n",
    "        'Precision': best_cv_results['SVM']['precision']['mean'],\n",
    "        'Recall': best_cv_results['SVM']['recall']['mean'],\n",
    "        'F1-Score': best_cv_results['SVM']['f1']['mean']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "compare_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Reshape data for visualization\n",
    "compare_melted = pd.melt(compare_df, id_vars='Model', \n",
    "                         value_vars=['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "                         var_name='Metric', value_name='Score')\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create grouped bar chart\n",
    "ax = sns.barplot(data=compare_melted, x='Metric', y='Score', hue='Model', palette='Set2')\n",
    "\n",
    "plt.title('Performance Comparison: Bernoulli NB (Raw) vs. SVM (Preprocessed)', fontsize=16)\n",
    "plt.xlabel('Metric', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.ylim([0.4, 1.0])\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Model')\n",
    "\n",
    "# Add value labels on bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.3f}', \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'bottom',\n",
    "                fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical significance test (if applicable)\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(compare_df.set_index('Model'))\n",
    "\n",
    "# Analyze differences\n",
    "diff_accuracy = abs(comparison_data[0]['Accuracy'] - comparison_data[1]['Accuracy'])\n",
    "diff_f1 = abs(comparison_data[0]['F1-Score'] - comparison_data[1]['F1-Score'])\n",
    "print(f\"\\nDifference in Accuracy: {diff_accuracy:.3f}\")\n",
    "print(f\"Difference in F1-Score: {diff_f1:.3f}\")\n",
    "\n",
    "# Print conclusions\n",
    "if comparison_data[0]['Accuracy'] > comparison_data[1]['Accuracy']:\n",
    "    winner = \"Bernoulli NB with raw text\"\n",
    "else:\n",
    "    winner = \"SVM with preprocessed text\"\n",
    "\n",
    "print(f\"\\nConclusion: {winner} performs better in terms of accuracy.\")\n",
    "print(\"This suggests that \" + \n",
    "      (\"text preprocessing is beneficial for this classification task.\" \n",
    "       if winner == \"SVM with preprocessed text\" \n",
    "       else \"raw text features might contain important signals that are lost during preprocessing.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best_models['SVM']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569edfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predictions from the best model on the test set\n",
    "y_pred = best_model.predict(X_test_task1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test_task1, y_pred)\n",
    "\n",
    "# Create a prettier visualization of the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale=1.4)\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                 xticklabels=['Not Science', 'Science'],\n",
    "                 yticklabels=['Not Science', 'Science'])\n",
    "\n",
    "# Add labels, title and ticks\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix - SVM Model')\n",
    "\n",
    "# Calculate metrics\n",
    "report = classification_report(y_test_task1, y_pred, output_dict=True)\n",
    "accuracy = (cm[0, 0] + cm[1, 1]) / np.sum(cm)\n",
    "precision = report['1']['precision']\n",
    "recall = report['1']['recall']\n",
    "f1 = report['1']['f1-score']\n",
    "\n",
    "# Display metrics on the plot\n",
    "plt.figtext(0.5, 0.01, f'Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1-Score: {f1:.4f}', \n",
    "            ha='center', fontsize=12, bbox={'facecolor': 'lightblue', 'alpha': 0.5, 'pad': 5})\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install imbalanced-learn\n",
    "\n",
    "# Import required modules\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e68a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Task 1 (science_related classification)\n",
    "def apply_undersampling(X, y):\n",
    "    undersampler = RandomUnderSampler(random_state=42)\n",
    "    X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "    print(f\"Original distribution: {Counter(y)}\")\n",
    "    print(f\"Distribution after undersampling: {Counter(y_resampled)}\")\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Apply to Task 1\n",
    "X_train_task1_under, y_train_task1_under = apply_undersampling(X_train_task1, y_train_task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e729a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effect of sampling\n",
    "def plot_sampling_comparison(original_y, resampled_y, title):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(x=original_y)\n",
    "    plt.title('Original Distribution')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.countplot(x=resampled_y)\n",
    "    plt.title('After Resampling')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example for Task 1\n",
    "plot_sampling_comparison(y_train_task1, y_train_task1_under, 'Effect of Undersampling on Task 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the best model (SVM) with undersampled data\n",
    "best_model = best_models['SVM']\n",
    "\n",
    "# Evaluate on the original test data for fair comparison\n",
    "best_model.fit(X_train_task1_under, y_train_task1_under)\n",
    "y_pred_under = best_model.predict(X_test_task1)\n",
    "accuracy_under = accuracy_score(y_test_task1, y_pred_under)\n",
    "report_under = classification_report(y_test_task1, y_pred_under, output_dict=True)\n",
    "cm_under = confusion_matrix(y_test_task1, y_pred_under)\n",
    "\n",
    "# Perform k-fold cross validation with undersampled data\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Store evaluation metrics\n",
    "fold_accuracy_under = []\n",
    "fold_precision_under = []\n",
    "fold_recall_under = []\n",
    "fold_f1_under = []\n",
    "fold_auc_under = []\n",
    "\n",
    "# Get a balanced dataset\n",
    "X_balanced = X_train_task1_under\n",
    "y_balanced = y_train_task1_under\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X_balanced, y_balanced)):\n",
    "    X_train_fold, X_test_fold = X_balanced[train_idx], X_balanced[test_idx]\n",
    "    y_train_fold, y_test_fold = y_balanced.iloc[train_idx], y_balanced.iloc[test_idx]\n",
    "    \n",
    "    best_model.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_fold = best_model.predict(X_test_fold)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_test_fold, y_pred_fold)\n",
    "    prec = precision_score(y_test_fold, y_pred_fold, zero_division=0)\n",
    "    rec = recall_score(y_test_fold, y_pred_fold, zero_division=0)\n",
    "    f1 = f1_score(y_test_fold, y_pred_fold, zero_division=0)\n",
    "    \n",
    "    # For AUC\n",
    "    if hasattr(best_model, \"predict_proba\"):\n",
    "        y_prob = best_model.predict_proba(X_test_fold)[:, 1]\n",
    "        auc_score = roc_auc_score(y_test_fold, y_prob)\n",
    "        fold_auc_under.append(auc_score)\n",
    "    \n",
    "    fold_accuracy_under.append(acc)\n",
    "    fold_precision_under.append(prec)\n",
    "    fold_recall_under.append(rec)\n",
    "    fold_f1_under.append(f1)\n",
    "\n",
    "# Compare performance metrics before and after undersampling\n",
    "comparison_data = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC'],\n",
    "    'Original': [\n",
    "        best_cv_results['SVM']['accuracy']['mean'],\n",
    "        best_cv_results['SVM']['precision']['mean'], \n",
    "        best_cv_results['SVM']['recall']['mean'],\n",
    "        best_cv_results['SVM']['f1']['mean'],\n",
    "        best_cv_results['SVM']['auc']['mean'] if 'auc' in best_cv_results['SVM'] else None\n",
    "    ],\n",
    "    'Undersampled': [\n",
    "        np.mean(fold_accuracy_under),\n",
    "        np.mean(fold_precision_under),\n",
    "        np.mean(fold_recall_under),\n",
    "        np.mean(fold_f1_under),\n",
    "        np.mean(fold_auc_under) if fold_auc_under else None\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Create visualizations\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Bar chart comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "comparison_melted = pd.melt(comparison_df, id_vars=['Metric'], \n",
    "                           value_vars=['Original', 'Undersampled'],\n",
    "                           var_name='Data Balance', value_name='Score')\n",
    "\n",
    "# Filter out any None values\n",
    "comparison_melted = comparison_melted.dropna()\n",
    "\n",
    "sns.barplot(x='Metric', y='Score', hue='Data Balance', data=comparison_melted)\n",
    "plt.title('SVM Performance: Original vs. Undersampled', fontsize=14)\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Confusion matrix for undersampled model\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.heatmap(cm_under, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Not Science', 'Science'],\n",
    "            yticklabels=['Not Science', 'Science'])\n",
    "plt.title('Confusion Matrix (Undersampled)', fontsize=14)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# ROC Curve comparison if available\n",
    "if hasattr(best_model, \"predict_proba\"):\n",
    "    plt.subplot(2, 2, 3)\n",
    "    # For original model\n",
    "    y_prob_orig = best_model.fit(X_train_task1, y_train_task1).predict_proba(X_test_task1)[:, 1]\n",
    "    fpr_orig, tpr_orig, _ = roc_curve(y_test_task1, y_prob_orig)\n",
    "    auc_orig = auc(fpr_orig, tpr_orig)\n",
    "    \n",
    "    # For undersampled model\n",
    "    best_model.fit(X_train_task1_under, y_train_task1_under)\n",
    "    y_prob_under = best_model.predict_proba(X_test_task1)[:, 1]\n",
    "    fpr_under, tpr_under, _ = roc_curve(y_test_task1, y_prob_under)\n",
    "    auc_under = auc(fpr_under, tpr_under)\n",
    "    \n",
    "    # Plot ROC curves\n",
    "    plt.plot(fpr_orig, tpr_orig, label=f'Original (AUC = {auc_orig:.3f})')\n",
    "    plt.plot(fpr_under, tpr_under, label=f'Undersampled (AUC = {auc_under:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title('ROC Curve Comparison', fontsize=14)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Class distribution before and after undersampling\n",
    "plt.subplot(2, 2, 4)\n",
    "class_dist = pd.DataFrame({\n",
    "    'Dataset': ['Original Train'] * 2 + ['Undersampled Train'] * 2,\n",
    "    'Class': ['Not Science', 'Science'] * 2,\n",
    "    'Count': [\n",
    "        sum(y_train_task1 == 0), \n",
    "        sum(y_train_task1 == 1),\n",
    "        sum(y_train_task1_under == 0), \n",
    "        sum(y_train_task1_under == 1)\n",
    "    ]\n",
    "})\n",
    "sns.barplot(x='Dataset', y='Count', hue='Class', data=class_dist)\n",
    "plt.title('Class Distribution Before vs After Undersampling', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nSVM Performance Comparison Summary:\")\n",
    "print(comparison_df.set_index('Metric'))\n",
    "print(\"\\nOriginal Class Distribution:\", Counter(y_train_task1))\n",
    "print(\"Undersampled Class Distribution:\", Counter(y_train_task1_under))\n",
    "\n",
    "# Calculate improvement percentages\n",
    "improvement_df = pd.DataFrame({\n",
    "    'Metric': comparison_df['Metric'],\n",
    "    'Improvement (%)': [\n",
    "        ((comparison_df['Undersampled'][i] - comparison_df['Original'][i]) / comparison_df['Original'][i]) * 100 \n",
    "        if comparison_df['Original'][i] is not None and comparison_df['Undersampled'][i] is not None\n",
    "        else None\n",
    "        for i in range(len(comparison_df))\n",
    "    ]\n",
    "})\n",
    "print(\"\\nRelative Improvement After Undersampling:\")\n",
    "print(improvement_df.set_index('Metric'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
